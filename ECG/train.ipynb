{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89ce36-06e9-4c11-af1f-a4b4c5dc9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self, args):\n",
    "        # dataset parameters\n",
    "        self.dataset = args.dataset\n",
    "        self.seed_id = args.seed_id\n",
    "\n",
    "        self.device = torch.device(args.device)\n",
    "\n",
    "        # Exp Description\n",
    "        self.run_description = f\"{args.run_description}_{datetime.datetime.now().strftime('%H_%M')}\"\n",
    "        self.experiment_description = args.experiment_description\n",
    "\n",
    "        # paths\n",
    "        self.home_path = os.getcwd()\n",
    "        self.save_dir = os.path.join(os.getcwd(), \"experiments_logs\")\n",
    "        self.exp_log_dir = os.path.join(self.save_dir, self.experiment_description, self.run_description)\n",
    "        os.makedirs(self.exp_log_dir, exist_ok=True)\n",
    "\n",
    "        self.data_path = args.data_path\n",
    "\n",
    "\n",
    "        # Specify runs\n",
    "        self.num_runs = args.num_runs\n",
    "\n",
    "        # get dataset and base model configs\n",
    "        self.dataset_configs, self.hparams_class = self.get_configs()\n",
    "\n",
    "        # Specify hparams\n",
    "        self.hparams = self.hparams_class.train_params\n",
    "\n",
    "    def get_configs(self):\n",
    "        dataset_class = get_dataset_class(self.dataset)\n",
    "        hparams_class = get_hparams_class(\"supervised\")\n",
    "        return dataset_class(), hparams_class()\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        self.train_dl, self.val_dl, self.test_dl, self.cw_dict = \\\n",
    "            data_generator(self.data_path, data_type, self.hparams)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def calc_results_per_run(self):\n",
    "        acc, f1 = _calc_metrics(self.pred_labels, self.true_labels, self.dataset_configs.class_names)\n",
    "        return acc, f1\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        self.metrics = {'accuracy': [], 'f1_score': []}\n",
    "\n",
    "        # fixing random seed\n",
    "        fix_randomness(int(self.seed_id))\n",
    "\n",
    "        # Logging\n",
    "        self.logger, self.scenario_log_dir = starting_logs(self.dataset, self.exp_log_dir, self.seed_id)\n",
    "        self.logger.debug(self.hparams)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data(self.dataset)\n",
    "\n",
    "        model = Model(configs=self.dataset_configs, hparams=self.hparams)\n",
    "        model.to(self.device)\n",
    "\n",
    "        # Average meters\n",
    "        loss_avg_meters = collections.defaultdict(lambda: AverageMeter())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "            betas=(0.9, 0.99)\n",
    "        )\n",
    "        \n",
    "        weights = [float(value) for value in self.cw_dict.values()]\n",
    "        # Now convert the list of floats to a numpy array, then to a PyTorch tensor\n",
    "        weights_array = np.array(weights).astype(np.float32)  # Ensuring the correct dtype\n",
    "        weights_tensor = torch.tensor(weights_array).to(self.device)\n",
    "        self.cross_entropy = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "        best_acc = 0\n",
    "        best_f1 = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = [] \n",
    "        val_accuracies = []\n",
    "\n",
    "\n",
    "        # 初始化早停对象\n",
    "        # early_stopping = EarlyStopping(patience=10, verbose=True, delta=0.001)\n",
    "        # training..\n",
    "        for epoch in range(1, self.hparams[\"num_epochs\"] + 1):\n",
    "            model.train()\n",
    "            \n",
    "            for step, batches in enumerate(self.train_dl):\n",
    "                batches = to_device(batches, self.device)\n",
    "\n",
    "                data = batches['samples'].float()\n",
    "                labels = batches['labels'].long()\n",
    "\n",
    "                # ====== Source =====================\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Src original features\n",
    "                logits = model(data)\n",
    "\n",
    "                # Cross-Entropy loss\n",
    "                # x_ent_loss = self.cross_entropy(logits, labels)\n",
    "                x_ent_loss = self.cross_entropy(logits, labels)\n",
    "                x_ent_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses = {'Total_loss': x_ent_loss.item()}\n",
    "                for key, val in losses.items():\n",
    "                    loss_avg_meters[key].update(val, self.hparams[\"batch_size\"])\n",
    "\n",
    "            self.evaluate(model, self.train_dl)\n",
    "            tr_acc, tr_f1 = self.calc_results_per_run()\n",
    "\n",
    "\n",
    "            train_accuracies.append(tr_acc/100)\n",
    "            # train_losses.append(self.trg_loss.cpu().numpy())\n",
    "            train_losses.append(self.trg_loss.cpu().numpy())\n",
    "\n",
    "            \n",
    "            # logging\n",
    "            self.logger.debug(f'[Epoch : {epoch}/{self.hparams[\"num_epochs\"]}]')\n",
    "            for key, val in loss_avg_meters.items():\n",
    "                self.logger.debug(f'{key}\\t: {val.avg:2.4f}')\n",
    "            self.logger.debug(f'TRAIN: Acc:{tr_acc:2.4f} \\t F1:{tr_f1:2.4f}')\n",
    "\n",
    "            # VALIDATION part\n",
    "            self.evaluate(model, self.val_dl)\n",
    "            ts_acc, ts_f1 = self.calc_results_per_run()\n",
    "            val_accuracies.append(ts_acc/100)\n",
    "            val_losses.append(self.trg_loss.cpu().numpy())\n",
    "            \n",
    "\n",
    "            if ts_f1 > best_f1:  # save best model based on best f1.\n",
    "                best_f1 = ts_f1\n",
    "                best_acc = ts_acc\n",
    "                save_checkpoint(self.exp_log_dir, model, self.dataset, self.dataset_configs, self.hparams, \"best\")\n",
    "                _save_metrics(self.pred_labels, self.true_labels, self.exp_log_dir, \"validation_best\")\n",
    "\n",
    "\n",
    "            \n",
    "            # logging\n",
    "            self.logger.debug(f'VAL  : Acc:{ts_acc:2.4f} \\t F1:{ts_f1:2.4f} (best: {best_f1:2.4f})')\n",
    "            self.logger.debug(f'-------------------------------------')\n",
    "\n",
    "        \n",
    "            # LAST EPOCH\n",
    "        _save_metrics(self.pred_labels, self.true_labels, self.exp_log_dir, \"validation_last\")\n",
    "        self.logger.debug(\"LAST EPOCH PERFORMANCE on validation set...\")\n",
    "        self.logger.debug(f'Acc:{ts_acc:2.4f} \\t F1:{ts_f1:2.4f}')\n",
    "\n",
    "        self.logger.debug(\":::::::::::::\")\n",
    "        # BEST EPOCH\n",
    "        self.logger.debug(\"BEST EPOCH PERFORMANCE on validation set ...\")\n",
    "        self.logger.debug(f'Acc:{best_acc:2.4f} \\t F1:{best_f1:2.4f}')\n",
    "        save_checkpoint(self.exp_log_dir, model, self.dataset, self.dataset_configs, self.hparams, \"last\")\n",
    "\n",
    "\n",
    "        # TESTING\n",
    "        print(\" === Evaluating on TEST set ===\")\n",
    "        self.evaluate(model, self.test_dl)\n",
    "        test_acc, test_f1 = self.calc_results_per_run()\n",
    "        _save_metrics(self.pred_labels, self.true_labels, self.exp_log_dir, \"test_last\")\n",
    "        self.logger.debug(f'Acc:{test_acc:2.4f} \\t F1:{test_f1:2.4f}')\n",
    "        class_names = ['N', 'S', 'V', 'F', 'Q']\n",
    "        plot_confusion_matrix(self.true_labels,self.pred_labels, self.dataset_configs.class_names )\n",
    "        plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "        np.save('train_losses.npy', train_losses)\n",
    "        np.save('val_losses.npy', val_losses)\n",
    "        np.save('train_accuracies.npy', train_accuracies)\n",
    "        np.save('val_accuracies.npy', val_accuracies)\n",
    "        \n",
    "\n",
    "    def evaluate(self, model, dataset):\n",
    "        model.to(self.device).eval()\n",
    "\n",
    "        total_loss_ = []\n",
    "\n",
    "        self.pred_labels = np.array([])\n",
    "        self.true_labels = np.array([])\n",
    "        self.trg_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batches in dataset:\n",
    "                batches = to_device(batches, self.device)\n",
    "                data = batches['samples'].float()\n",
    "                labels = batches['labels'].long()\n",
    "\n",
    "                # forward pass\n",
    "                predictions = model(data)\n",
    " \n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(predictions, labels)\n",
    "                # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "                \n",
    "                # loss +=0.001 * l2_norm\n",
    "                total_loss_.append(loss.item())\n",
    "                pred = predictions.detach().argmax(dim=1)  # get the index of the max log-probability\n",
    "\n",
    "                self.pred_labels = np.append(self.pred_labels, pred.cpu().numpy())\n",
    "                self.true_labels = np.append(self.true_labels, labels.data.cpu().numpy())\n",
    "\n",
    "        self.trg_loss = torch.tensor(total_loss_).mean()  # average loss无用\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
